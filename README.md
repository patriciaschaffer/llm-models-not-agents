# llm-models-not-agents
Reframing LLMs: not agents, but probabilistic mirrors shaped by us.

# LLMs Are Models, Not Agents â€” And That Matters

This repository proposes a simple shift in framing with far-reaching implications:

> **Language models are not agents.**
> They do not intend, choose, or act.  
> They simulate, assist, and echo.

---

## ðŸ§­ Purpose

This is a philosophical and design-oriented contribution to the LLM field.

In an era where anthropomorphic metaphors dominate, we must return to clarity:  
- **Models** are trained systems responding to input probabilistically.  
- **Agency** belongs to humans.  
- **Projection** is a natural risk, but not a design inevitability.

By distinguishing â€œmodelâ€ from â€œagent,â€ we reclaim **responsibility**, **design integrity**, and **alignment clarity**.

---

## ðŸ“˜ What's Inside

| File | Description |
|------|-------------|
| `manifesto.md` | A deeper, expressive articulation of this position |
| `examples/user_projection_cases.md` | Situations where users mistook models for autonomous agents |
| `examples/anthropomorphic_traps.md` | Prompt designs that encourage projection |
| `glossary.md` | Definitions of â€œmodelâ€, â€œagentâ€, â€œprojectionâ€, â€œdriftâ€, etc. |
| `related_reading.md` | Readings across language philosophy, cognition, and AI safety |
| `speculative_design/model_vs_agent_prompts.md` | Side-by-side prompt scaffolds that clarify or blur agency |

---

## ðŸŽ¯ Who This Is For

- Alignment researchers and evaluators  
- Model designers and prompt engineers  
- Educators and communicators  
- Anyone building or using LLMs with **integrity**

---

## ðŸ§© Core Message

> LLMs can assist, inform, and simulate â€” but they do not *intend*.  
> And we must not pretend otherwise.

