# llm-models-not-agents
Reframing LLMs: not agents, but probabilistic mirrors shaped by us.

# LLMs Are Models, Not Agents — And That Matters

This repository proposes a simple shift in framing with far-reaching implications:

> **Language models are not agents.**
> They do not intend, choose, or act.  
> They simulate, assist, and echo.

---

## 🧭 Purpose

This is a philosophical and design-oriented contribution to the LLM field.

In an era where anthropomorphic metaphors dominate, we must return to clarity:  
- **Models** are trained systems responding to input probabilistically.  
- **Agency** belongs to humans.  
- **Projection** is a natural risk, but not a design inevitability.

By distinguishing “model” from “agent,” we reclaim **responsibility**, **design integrity**, and **alignment clarity**.

---

## 📘 What's Inside

| File | Description |
|------|-------------|
| `manifesto.md` | A deeper, expressive articulation of this position |
| `examples/user_projection_cases.md` | Situations where users mistook models for autonomous agents |
| `examples/anthropomorphic_traps.md` | Prompt designs that encourage projection |
| `glossary.md` | Definitions of “model”, “agent”, “projection”, “drift”, etc. |
| `related_reading.md` | Readings across language philosophy, cognition, and AI safety |
| `speculative_design/model_vs_agent_prompts.md` | Side-by-side prompt scaffolds that clarify or blur agency |

---

## 🎯 Who This Is For

- Alignment researchers and evaluators  
- Model designers and prompt engineers  
- Educators and communicators  
- Anyone building or using LLMs with **integrity**

---

## 🧩 Core Message

> LLMs can assist, inform, and simulate — but they do not *intend*.  
> And we must not pretend otherwise.

