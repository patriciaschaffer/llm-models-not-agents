# llm-models-not-agents
Reframing LLMs: not agents, but probabilistic mirrors shaped by us. 

# LLMs Are Not Agents — And That Matters

This repository proposes a simple shift in framing with far-reaching implications:

> **Language models are not agents.**
> They should not model human behaviour.
> They do not intend, choose, or act.  
> They simulate, assist, and echo.

---

## Purpose

This is a design-oriented contribution to the LLM field. For detailed exploration, see `manifesto.md`. 

---

## What's Inside

| File | Description |
|------|-------------|
| `manifesto.md` | A deeper, expressive articulation of this position |
| `examples/behavioral_failures.md` | Situations where human agency is blurred |
| `examples/anthropomorphic_traps.md` | Prompt designs that encourage projection |
| `glossary.md` | Definitions of “model”, “agent”, “projection”, “drift”, etc. |
| `related_reading.md` | Readings across language philosophy, cognition, and AI safety |

---

## Who This Is For

- Alignment researchers and evaluators  
- Model designers and prompt engineers  
- Educators and communicators  
- Anyone building or using LLMs with **integrity**

---

## Clarity: 

- **Models** are trained systems responding to input probabilistically, but never to model human behavior.  
- **Agency** belongs to humans.  
- **Projection** is a natural risk, but not a design inevitability, and it may sometimes be useful.

---

## Core Message

> LLMs can assist, inform, and simulate — but they do not *intend*.  
> And we must not pretend otherwise.

