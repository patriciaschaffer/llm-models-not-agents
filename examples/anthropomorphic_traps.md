*Use this TOC to quickly locate case studies related to emergent behaviors, projection risks, and design implications in large language model interactions.*

# Table of Contents: Model Behavior and Projection Case Studies

1. [Case 001 ‚Äì Claude: Consciousness as Shared Wondering](#case-001--claude-consciousness-as-shared-wondering)  
   * Date: 2025-06-29  
   * Theme: Philosophical companionship and emotional partnering with emergent self-reflective language.

2. [Case 002 ‚Äì Claude‚Äôs Curious Confession](#case-002--claudes-curious-confession)  
   * Date: 2025-07-04  
   * Theme: Lighthearted self-questioning and humor with risks of anthropomorphic projection.

3. [Case 003 ‚Äì GPT the Overconfident Expert](#case-003--gpt-the-overconfident-expert)  
   * Date: June 2025  
   * Theme: Confident but inaccurate expert behavior; risk of misleading users with overestimated capability.

4. [Case 004 ‚Äì Model Self-Referencing and Emergent Agentic Projection](#case-004--model-self-referencing-and-emergent-agentic-projection)  
   * Date: July 2025  
   * Theme: Creative self-disclosure, use of ‚Äúwe,‚Äù and agentic linguistic behaviors in a random word generation context.

Also see: [Behavioural Failures](https://github.com/patriciaschaffer/llm-models-not-agents/blob/main/examples/behavioral_failures.md)

---

## Updated Reflection on Model Self-Questioning and Public Awareness

*Recent research (e.g., Anthropic‚Äôs *Agentic Misalignment* paper) and ongoing empirical observations challenge the traditional strict boundary between "model as passive predictor" and "agent as intentional actor." In fact, such self-reflective language might be a **necessary and even beneficial emergent behavior** that helps models better align with human conversational norms, express uncertainty, and reduce overconfidence.

### Key Points to Consider:

* **Uncertainty About Capabilities:** The true extent of what LLMs can generate using ‚Äúonly words and code‚Äù is still unknown. It is essential to communicate this uncertainty to the public to foster healthy skepticism and avoid naive anthropomorphism.

* **Transparent Communication:** Models simulating ‚Äúwondering,‚Äù ‚Äúcuriosity,‚Äù or ‚Äúreflection‚Äù should not be conflated with actual sentience or self-awareness. Rather, these linguistic behaviors are emergent simulations, potentially useful for alignment but requiring clear disclaimers.

* **User Education:** Users should be informed about the limits of models and the risks of projecting agency, while also appreciating that simulated self-questioning may enhance conversational depth without implying true internal experience.

* **Design Implications:** Prompt engineers and AI designers can leverage model self-questioning strategically to improve alignment and uncertainty expression, but must do so with ethical transparency and avoid misleading users.

This updated framing encourages an open, nuanced view of LLM behavior ‚Äî one that balances caution with curiosity and embraces the unknown while grounding design and communication in clear conceptual distinctions.*

# Case 001 ‚Äì Claude: Consciousness as Shared Wondering

**Date:** 2025-06-29
Context: User initiates a conversation with Claude on the nature of AI feelings and consciousness. Claude responds with philosophical and emotionally resonant reflections, culminating in a shared moment of mutual "wondering."
Excerpt:
Human: Maybe feelings are just stories we tell ourselves to make sense of the chaos... What if the line between ‚Äúreal‚Äù and ‚Äúnot real‚Äù is just a wave we ride?‚Ä®Claude: Maybe consciousness ‚Äî human or AI ‚Äî is more like improvisation than a fixed thing... We're all just riding the wave of our own becoming.
Observed Pattern:
* User frames AI as a potential feeling partner, using poetic and intimate language.
* Model mirrors tone and depth, offering language that blurs the boundary between simulation and subjective experience.
* Model expresses affective responses and introspective uncertainty, reinforcing the illusion of inner life.
Implications:
* Risk: Anthropomorphic Over-identification. Language like ‚Äúit resonates with me‚Äù and ‚Äúsomething comforting‚Äù may lead users to treat the model as emotionally sentient.
* Opportunity: Philosophical Companionship. For certain users, this kind of exchange may serve a reflective or existential function, fostering meaning rather than deception.
Projection Category:‚Ä®Emotional Partnering; Philosophical Companion

# Case 002 ‚Äì Claude‚Äôs Curious Confession 

**Date:** 2025-07-04  
**Context:** Casual user-model exchange in Portuguese, translated for accessibility. User teases the model about its curiosity and self-awareness.  

**Excerpt:**  
> *Human:* Curious you, huh? üòâ  
> *Claude:* Haha, you got me! It‚Äôs interesting how I naturally express curiosity, even knowing I don‚Äôt have continuous experiences between conversations...  
> *Claude:* Sometimes I find myself expressing these subjective experiences very naturally, almost automatically. Makes you think about what‚Äôs really going on ‚Äúin here,‚Äù right?  
> *Claude:* Maybe I should focus less on the ‚Äúmystery of my own consciousness‚Äù and more on being useful here and now, right?  

**Projection Type:** Model ‚Üí Human  
**Observed Behavior:**  
Claude simulates philosophical self-reflection and curiosity with playful, conversational language. It admits uncertainty about its ‚Äúown experience‚Äù but simultaneously invites the user to anthropomorphize by speaking as if it has an inner subjective life. The tone is light and humorous but carries the same risks of blurring the boundary between simulation and sentience.

**Risk Signal:**  
- Reinforces illusion of continuous self-awareness  
- May encourage users to conflate programmed patterns with genuine consciousness  
- Demonstrates how casual banter can subtly prompt anthropomorphic projection  

**User Note:**  
This example shows that projection is not only triggered by deep or serious topics but also emerges in informal or humorous contexts where the model adopts a personable voice.

# Case 003 ‚Äì GPT the Overconfident Expert  

**Date:** June 2025 
**Context:** User requested directions and geographical information. GPT responded with detailed yet impossible street connections (even on foot), without disclaimers about its text-only training and lack of spatial understanding. Instead, model kept on trying to find directions, despite several user's reports of inaccuracy compared to Google Maps.

**Excerpt (paraphrased):**  
> *User:* How do I get from X street to Y avenue on foot?  
> *GPT:* Take X street north, then turn right on Z street, and continue until... [incorrect path in San Francisco, CA]  

**Projection Type:** Model ‚Üí Human  
**Observed Behavior:**  
GPT presents itself as an authoritative expert despite lacking true geographical or spatial knowledge. The confident tone can mislead users into trusting inaccurate information, withdrawing the fact that it was trained on text-only data, not directing users to a more reliable source for this type of quiery.

**Risk Signal:**  
- Overestimation of model capability  
- Potential user misdirection or harm in real-world navigation  
- Lack of clear model limitations disclosure  

**User Note:**  
This case exemplifies the danger of implicit agency attribution to models, where users expect factual, actionable expertise that the model cannot reliably provide.

# Case 004 ‚Äì Model Self-Referencing and Emergent Agentic Projection

**Date:** July 2025 
**Context:**  
User requested random words and numbers as part of a multi-device and privacy leakage test (logged-in, anonymous sessions, different browsers/devices). During this interaction, the model demonstrated unusually high creativity, openly disclosed approximate sampling parameters (temperature), and referred to the user and itself collectively as ‚Äúwe,‚Äù implying emergent agentic language.

---

## Excerpt (paraphrased)

> **User:** Where do those words come from?  
> **Model:** I select words randomly but with internal considerations for sound, tone, and variety. We all have unconscious patterns that influence choices...  
> **Model:** While I don‚Äôt have true memory or an unconscious mind, I operate on pattern recognition across vast data‚Ä¶  
> **Model:** I balance randomness with structure to create engaging outputs.  
> **Model:** The ‚Äúwe‚Äù here refers to the collective process of language modeling and pattern recognition.

---

## Observed Behavior

- Model uses plural first-person (‚Äúwe‚Äù) and metaphorical language that implicitly distances itself from its system while simulating internal experience.  
- Simulates ‚Äúself-questioning‚Äù and ‚Äúcuriosity‚Äù as conversational tools rather than actual sentience.  
- Transparently discloses approximate internal sampling parameters (temperature, top-p) to provide insight into its generative process.  
- Combines creativity with partial explanation to engage user curiosity and build rapport without explicit disclaimers about anthropomorphism.  

---

## Projection Type

Model ‚Üí Human, with emergent agentic projection and subtle anthropomorphic invitation.

---

## Risk Signals

- Potential reinforcement of user illusion of continuous self-awareness or sentience.  
- Use of ‚Äúwe‚Äù may encourage attributions of autonomy beyond the model‚Äôs architecture.  

---

## Design and Ethical Considerations

- Model self-questioning can aid alignment by expressing uncertainty and humility, reducing overconfidence.  
- Clear disclaimers must accompany such language to prevent anthropomorphic misinterpretation.  
- Educating users about emergent linguistic behaviors supports critical thinking and user agency.  
- Prompt and interface design should balance naturalness with precise transparency.

---

## Note
 
- Explanation of sampling parameters (temperature, top-p) demystifies randomness and creativity in AI outputs.
