# Glossary — Core Concepts

This glossary defines key terms used throughout the repository.  
Each term is selected for its relevance to framing, interpreting, and designing interactions with language models.

---

### Language Model (LLM)

A probabilistic system trained on vast text corpora to predict the next token given a prompt.  
It does not understand, intend, or choose. 
It simulates likely continuations based on patterns **and does not model human behavior**.

---

### Agent

An entity capable of **intentional action** — choosing goals, forming plans, and acting independently.  
LLMs are not agents. They may simulate agency, but they do not possess it.

---

### Agency
The capacity to set goals, make choices, and act with intent.
LLMs can mimic agency through language, but do not possess true agency.

---

### Projection

The psychological act of attributing human-like qualities (e.g., intention, emotion) to a system.  
LLMs often evoke projection due to their fluent, social outputs.

---

### Persona

A constructed identity or role a system adopts in interaction.
Personas guide tone, behavior, and perceived purpose — but remain surface-level simulations, not selves.

---

### Archetype

A universal pattern or role — like the Hero, Trickster, or Mentor — that recurs across stories and cultures.
Archetypes shape expectations and guide interpretation, including how users make sense of model outputs.

--- 

### Drift

When a user’s assumptions or framing about a model shift over time — often unconsciously — toward seeing it as agentic or sentient.  
Prompt drift can also refer to gradual changes in how prompts behave due to subtle updates or context changes.

---

### Anomaly

An unexpected or atypical model behavior or output that deviates significantly from the intended, normative, or prompt-aligned response patterns. Anomalies may include creative divergences, hallucinations, or failures to follow instructions that produce outputs outside the expected operational boundaries.

---

### Framing

The initial conceptual scaffolding we place around systems: are they tools, assistants, agents, friends?  
Framing affects how users interact, trust, and defer responsibility.

---

### Alignment

The degree to which a system’s behavior matches human values, intentions, or safety expectations.  
Clarifying the **non-agentic** nature of models is part of preserving alignment clarity.

---

### Responsibility Gap

When users or designers blame the model for outcomes they themselves shaped.  
Restoring human agency closes this gap.

---

### Grice’s Maxims

A set of conversational principles — Quantity, Quality, Relation, and Manner — that describe how humans typically cooperate in communication.
LLMs can follow these patterns statistically, but do not understand or intend them.

---
