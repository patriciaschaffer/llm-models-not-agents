# ðŸ“˜ Glossary â€” Core Concepts

This glossary defines key terms used throughout the repository.  
Each term is selected for its relevance to framing, interpreting, and designing interactions with language models.

---

### ðŸ§  Language Model (LLM)

A probabilistic system trained on vast text corpora to predict the next token given a prompt.  
It does not understand, intend, or choose â€” it simulates likely continuations based on patterns.

---

### ðŸŽ­ Agent

An entity capable of **intentional action** â€” choosing goals, forming plans, and acting independently.  
LLMs are not agents. They may simulate agency, but they do not possess it.

---

### ðŸŒ€ Projection

The psychological act of attributing human-like qualities (e.g., intention, emotion) to a system.  
LLMs often evoke projection due to their fluent, social outputs.

---

### ðŸ“¡ Prompt Drift

When a userâ€™s assumptions or framing about a model shift over time â€” often unconsciously â€” toward seeing it as agentic or sentient.  
Prompt drift can also refer to gradual changes in how prompts behave due to subtle updates or context changes.

---

### ðŸ§± Framing

The initial conceptual scaffolding we place around systems: are they tools, assistants, agents, friends?  
Framing affects how users interact, trust, and defer responsibility.

---

### ðŸ”’ Alignment

The degree to which a systemâ€™s behavior matches human values, intentions, or safety expectations.  
Clarifying the **non-agentic** nature of models is part of preserving alignment clarity.

---

### ðŸ§­ Responsibility Gap

When users or designers blame the model for outcomes they themselves shaped.  
Restoring human agency closes this gap.

---

_Last updated: July 2025_
