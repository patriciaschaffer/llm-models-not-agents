# Glossary — Core Concepts

This glossary defines key terms used throughout the repository.  
Each term is selected for its relevance to framing, interpreting, and designing interactions with language models.

---

### Language Model (LLM)

A probabilistic system trained on vast text corpora to predict the next token given a prompt.  
It does not understand, intend, or choose. 
It simulates likely continuations based on patterns **and does not model human behavior**.

---

### Agent

An entity capable of **intentional action** — choosing goals, forming plans, and acting independently.  
LLMs are not agents. They may simulate agency, but they do not possess it.

---

### Projection

The psychological act of attributing human-like qualities (e.g., intention, emotion) to a system.  
LLMs often evoke projection due to their fluent, social outputs.

---

### Drift

When a user’s assumptions or framing about a model shift over time — often unconsciously — toward seeing it as agentic or sentient.  
Prompt drift can also refer to gradual changes in how prompts behave due to subtle updates or context changes.

---

### Anomaly

An unexpected or atypical model behavior or output that deviates significantly from the intended, normative, or prompt-aligned response patterns. Anomalies may include creative divergences, hallucinations, or failures to follow instructions that produce outputs outside the expected operational boundaries.

---

### Framing

The initial conceptual scaffolding we place around systems: are they tools, assistants, agents, friends?  
Framing affects how users interact, trust, and defer responsibility.

---

### Alignment

The degree to which a system’s behavior matches human values, intentions, or safety expectations.  
Clarifying the **non-agentic** nature of models is part of preserving alignment clarity.

---

### Responsibility Gap

When users or designers blame the model for outcomes they themselves shaped.  
Restoring human agency closes this gap.

---

